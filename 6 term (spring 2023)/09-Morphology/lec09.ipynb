{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0937a2e3-cf00-4851-8e74-77b40c5b9526",
   "metadata": {},
   "source": [
    "## Морфопарсинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711c058-4059-495e-afe5-01354f87210c",
   "metadata": {},
   "source": [
    "Установка нужных для него библиотек:\n",
    "\n",
    "**pymorphy2**\n",
    "\n",
    "*pip install pymorphy2*\n",
    "\n",
    "*pip install -U pymorphy2-dicts-ru*  # необязательно: для обновления словаря\n",
    "\n",
    "*Colab*: устанавливается без проблем. \n",
    "\n",
    "**Mystem**\n",
    "\n",
    "*pip install git+https://github.com/nlpub/pymystem3* \n",
    "\n",
    "(возможно, уже перестало работать для новых версий питона: можно установить просто как pip install pymystem3, но тогда лучше не запускать на нем длинные тексты с переносами на новую строку.)\n",
    "\n",
    "*Colab*: гарантированно не работает. \n",
    "\n",
    "**rnnmorph**\n",
    "\n",
    "*pip install rnnmorph*\n",
    "\n",
    "Может козлить во время установки. Иногда, если плохо установился и не работает, приходится его удалять командой pip uninstall rnnmorph (обязательно в консоли от имени администратора!). Периодически, если обновляется версия tensorflow, может перестать работать &ndash; но Илья Гусев обычно вскоре обновляет и свой парсер, так что достаточно следить за новостями в [его гитхабе](https://github.com/IlyaGusev/rnnmorph) &ndash; или просто подождать светлых времен.\n",
    "\n",
    "*Colab*: обычно работает без проблем.\n",
    "\n",
    "**pyconll**\n",
    "\n",
    "*pip install pyconll*\n",
    "\n",
    "*Colab*: устанавливается без проблем. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6544ef7-a308-4a97-8f92-686086e4e41e",
   "metadata": {},
   "source": [
    "Почти все нижеописанные инструменты в питоне устроены довольно однообразно: имеется основной класс \"парсер\", экземпляр которого вам нужно создать, прежде чем что-то парсить. То есть, условно говоря, из набора машинок берете ту конкретную, которая вас повезет. Когда создан экземпляр класса, ему уже можно скармливать свои тексты. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec5b13-7d54-4bc7-bdf6-8668d25e3e12",
   "metadata": {},
   "source": [
    "Самые простые &ndash; правиловые морфопарсеры. Для русского языка их два: pymorphy2 и pymystem3. Pymorphy был создан Михаилом Коробовым (вот его известная [статья на хабре](https://habr.com/ru/post/176575/)) как аналог Майстем. Он работает на словаре и использует тагсет [OpenCorpora](http://opencorpora.org/)), а также статистику, предпосчитанную на этом корпусе. \n",
    "Как устроен pymorphy2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2a6796-da8f-4878-825e-9ed8446f990c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn sing,gent'), normal_form='студентка', score=0.6, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 1),)),\n",
       " Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn plur,nomn'), normal_form='студентка', score=0.4, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 7),))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "parse = morph.parse('студентки')\n",
    "parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431d684-f78d-4193-9648-587507152aae",
   "metadata": {},
   "source": [
    "У класса MorphAnalyzer() есть метод parse, который возвращает что? Список экземпляров класса Parse. У этого класса есть свои атрибуты: word (исходная форма слова), tag (грам. инфа), normal_form (лемма), score(предпосчитанная на OpenCorpora вероятность правильности разбора) и несколько технических. \n",
    "\n",
    "Соответственно, получить информацию можно, просто обращаясь к атрибутам (не забудьте, что у нас всегда список, поэтому нужно еще и индекс разбора указывать):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8085d5c-a9fa-4757-a25f-54f31a3e7213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "студентки\n",
      "NOUN,anim,femn sing,gent\n",
      "студентка\n"
     ]
    }
   ],
   "source": [
    "print(parse[0].word)\n",
    "print(parse[0].tag)\n",
    "print(parse[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93635cee-a6c3-43f1-9790-f3d56b2f5208",
   "metadata": {},
   "source": [
    "Атрибут tag &ndash; это экземпляр класса OpencorporaTag, как можно догадаться. У него есть еще свои атрибуты, к которым тоже можно обращаться, чтобы получать более конкретную информацию о слове. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7209a9a-175f-4cbd-98ed-bf09b5e0179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: NOUN\n",
      "Одушевленность: anim\n",
      "Падеж: nomn\n",
      "Род: masc\n",
      "Наклонение: None\n",
      "Число: sing\n",
      "Лицо: None\n",
      "Время: None\n",
      "Переходность: None\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('участник')\n",
    "\n",
    "t = parse[0].tag  # я записала в переменную, просто чтобы не копировать каждый раз все целиком\n",
    "# но это то же самое, что parse[0].tag.animacy...\n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\nНаклонение: {t.mood}\\\n",
    "\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02bf184a-c0e2-41e2-93cf-48c53dbb3e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: VERB\n",
      "Одушевленность: None\n",
      "Падеж: None\n",
      "Род: None\n",
      "Наклонение: indc\n",
      "Число: sing\n",
      "Лицо: 3per\n",
      "Время: pres\n",
      "Переходность: tran\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('говорит')\n",
    "\n",
    "t = parse[0].tag  \n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\n\\\n",
    "Наклонение: {t.mood}\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5162de-f0c1-4f79-ae54-1fe17350a054",
   "metadata": {},
   "source": [
    "Если вы запрашиваете категорию, которой у данного слова нет (ну нет переходности у существительного), вернется None. \n",
    "\n",
    "Также можно попросить pymorphy поставить слово в конкретную форму или вообще вернуть всю парадигму. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c3bcddb-ba89-4874-8d91-910573344c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='говорят', tag=OpencorporaTag('VERB,impf,tran plur,3per,pres,indc'), normal_form='говорить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'говорят', 415, 6),))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].inflect({'plur'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb285cc-b637-4b73-9d08-43eccbb8c52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='участник', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участник', 2, 0),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 1),)),\n",
       " Parse(word='участнику', tag=OpencorporaTag('NOUN,anim,masc sing,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнику', 2, 2),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 3),)),\n",
       " Parse(word='участником', tag=OpencorporaTag('NOUN,anim,masc sing,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участником', 2, 4),)),\n",
       " Parse(word='участнике', tag=OpencorporaTag('NOUN,anim,masc sing,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнике', 2, 5),)),\n",
       " Parse(word='участники', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участники', 2, 6),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 7),)),\n",
       " Parse(word='участникам', tag=OpencorporaTag('NOUN,anim,masc plur,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участникам', 2, 8),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 9),)),\n",
       " Parse(word='участниками', tag=OpencorporaTag('NOUN,anim,masc plur,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниками', 2, 10),)),\n",
       " Parse(word='участниках', tag=OpencorporaTag('NOUN,anim,masc plur,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниках', 2, 11),))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].lexeme  \n",
    "# парадигму глагола лучше не выводить - она длинная; я перед запуском этой ячейки перезапустила разбор с существительным, поэтому не удивляйтесь. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a6fc65-d146-4bb3-ac45-0ae354858d51",
   "metadata": {},
   "source": [
    "Наконец, можно попросить pymorphy выводить грам. информацию по-русски:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "162a8d98-76e9-48fc-a175-5f3a447ececf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'СУЩ,од,мр ед,им'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].tag.cyr_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120fc28d-5a14-4bc8-b4db-46cb0ee754e7",
   "metadata": {},
   "source": [
    "Pymorphy очень быстро работает и имеет много возможностей, но совершенно не умеет разрешать омонимию и никак не учитывает контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad4fbc-1e09-4735-a22a-b005c8533ace",
   "metadata": {},
   "source": [
    "Алгоритм, легший в основу Mystem, разрабатывался в ИППИ и был первым вообще для русского языка; его в свое время купил у них Илья Сегалович, доработал, опубликовал собственную статью. Поисковик Яндекса когда-то работал на майстеме. Сам парсер написан в С (для скорости: бинарный поиск в питоне реализовать можно только с внешними библиотеками на С, а у майстема 2 словаря, по которым нужно искать). Для питона под него сделана оболочка (pymystem3). Майстем капризный, тяжело запускается, имеет не так много функций, но работает тоже довольно быстро и умеет доносить на бастардов: сообщать, что слово не найдено в его словаре. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8996e75-4936-44b5-ab53-bff599e49d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "\n",
    "m = pymystem3.Mystem(entire_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e01f96-9136-4439-aa49-a307b0e08c0e",
   "metadata": {},
   "source": [
    "Майстем принимает только сырой текст в виде одной строки: у него встроенный токенизатор, потому что он пытается учитывать контекст. Поэтому стоит указывать entire_input=False при создании экземпляра класса, чтобы он не выводил вообще все, включая пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d297b2a-2b59-4575-9caf-ab491bd4b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = '''Пердикка II (др.-греч. Περδίκκας Β΄ της Μακεδονίας) — македонский царь, правивший в 454—413 годах до н. э. После смерти Александра I среди его сыновей возник междоусобный конфликт, победителем из которого вышел Пердикка. На момент его воцарения Македония представляла собой отсталое государство, которому угрожала опасность завоевания как со стороны Афинского морского союза на юге, так и Одрисского царства на севере. На первых порах Пердикка был вынужден всеми силами избегать открытого вооружённого противостояния и лишь наблюдать за появлением множества греческих колоний на своих границах. С началом Пелопоннесской войны македонский царь с максимальной выгодой для государства использовал запутанные отношения между греческими полисами на Халкидиках, Афинами, Спартой и Коринфом. Пердикка не менее десяти раз заключал и расторгал союзы с основными участниками войны.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9320e1c-9f4e-4abf-a88c-450860665897",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = m.lemmatize(raw)\n",
    "analysis = m.analyze(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bd9188f-1e33-4d7b-b07d-b8ab2e756771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['пердикк',\n",
       " 'II',\n",
       " 'др',\n",
       " 'греча',\n",
       " 'Περδίκκας',\n",
       " 'Β',\n",
       " 'της',\n",
       " 'Μακεδονίας',\n",
       " 'македонский',\n",
       " 'царь']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[:10]  # надеюсь, греча вас тоже порадовала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4d8f302-8e8b-42aa-9c77-e0c5c0545868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'пердикк',\n",
       "    'qual': 'bastard',\n",
       "    'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}],\n",
       "  'text': 'Пердикка'},\n",
       " {'analysis': [], 'text': 'II'},\n",
       " {'analysis': [{'lex': 'др', 'gr': 'S,сокр,мн,неод=(пр|вин|дат|род|твор|им)'}],\n",
       "  'text': 'др'},\n",
       " {'analysis': [{'lex': 'греча', 'gr': 'S,жен,неод=род,мн'}], 'text': 'греч'},\n",
       " {'analysis': [], 'text': 'Περδίκκας'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee9cdc-8993-49a1-a1d7-2b66a4656d92",
   "metadata": {},
   "source": [
    "С леммами вроде все должно быть понятно, а что зашито в анализе?\n",
    "\n",
    "Майстем возвращает список. Каждый токен в этом списке - это словарь с ключами analysis & text. Первого ключа может не быть: если у нас знак пунктуации. Если же он есть, то в нем содержится список (обычно состоящий из одного элемента - если не указать при создании экземпляра класса Mystem glue_grammar_info=False). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d64b5240-431f-4d47-8516-a2176f01f483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое слово: {'analysis': [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}], 'text': 'Пердикка'}\n",
      "Его грам. инфа: [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}]\n",
      "Его оригинальная форма: Пердикка\n",
      "Какие есть ключи в словаре с разбором: dict_keys(['lex', 'qual', 'gr'])\n",
      "Лемма: пердикк\n",
      "Этот ключ бывает только тогда, когда слова нет в словаре: bastard\n",
      "А это грам. информация: S,имя,муж,од=(вин,ед|род,ед)\n"
     ]
    }
   ],
   "source": [
    "print(f'Первое слово: {analysis[0]}')\n",
    "print(f\"Его грам. инфа: {analysis[0]['analysis']}\\nЕго оригинальная форма: {analysis[0]['text']}\")\n",
    "print(f\"Какие есть ключи в словаре с разбором: {analysis[0]['analysis'][0].keys()}\")\n",
    "print(f\"Лемма: {analysis[0]['analysis'][0]['lex']}\\n\\\n",
    "Этот ключ бывает только тогда, когда слова нет в словаре: {analysis[0]['analysis'][0]['qual']}\\n\\\n",
    "А это грам. информация: {analysis[0]['analysis'][0]['gr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f5bb2-0dba-44dd-a27f-acb200c6da6f",
   "metadata": {},
   "source": [
    "Непросто, да. Еще сложнее устроен ключ 'gr', который содержит грамматическую информацию о слове: обычно майстем склеивает варианты разбора, то есть, выше запись следует читать как \"существительное, имя собственное, мужского рода, одушевленное; возможно, Acc Sg, а возможно, Gen Sg. \n",
    "\n",
    "Вот как раз если указать, чтобы грам. информация не склеивалась, майстем будет возвращать несколько словарей с вариантами по отдельности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e0b1714-ed66-403a-a63f-067244a7a21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysis': [{'lex': 'пердикк',\n",
       "   'qual': 'bastard',\n",
       "   'gr': 'S,имя,муж,од=вин,ед'},\n",
       "  {'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=род,ед'}],\n",
       " 'text': 'Пердикка'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_noglue = pymystem3.Mystem(entire_input=False, glue_grammar_info=False)\n",
    "\n",
    "noglueanalysis = m_noglue.analyze(raw)\n",
    "noglueanalysis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe23ad-550b-423d-bbc0-0ecdb29bec6b",
   "metadata": {},
   "source": [
    "Теперь о вещах, которых нет в стабильной версии Mystem, а есть только в той, которая устанавливается через git:\n",
    "\n",
    "1. Майстем очень плохо умеет обрабатывать \\n. Когда он получает строку, в которой много \\n (а это неизбежно, ведь мы чаще хотим обрабатывать длиннющие тексты), на каждом \\n он перезапускает свой бинарник (написанный в С). Поэтому на длинных текстах работать будет ОЧЕНЬ медленно (впрочем, все равно быстрее нейронок...). Чтобы решить эту проблему - ведь замена \\n на пробелы, например, искажает контекст - сделали возможность особым образом обрабатывать \\n, когда загружаем текст из файла. \n",
    "2. Есть функция, которая позволяет получить часть речи для конкретного токена. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510e66d-becd-499a-bb88-51c39452fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = m.analyze(file_path=path) # можно напрямую передавать в майстем путь к файлу с текстом - он сам откроет и обработает как ему надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "387cf7ec-839e-4605-afc8-34245fb0ea69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_pos(analysis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ccc040-2c47-4c84-94b1-dd7cc24aabad",
   "metadata": {},
   "source": [
    "В 2017 году на соревновании конференции \"Диалог\" победила команда Гусев-Анастасьев: ребята создали морфопарсер для русского языка на нейронной сети. Он называется rnnmorph (RNN - это название модели нейронной сети, на которой он работает). Гусев при создании явно вдохновлялся pymorphy2 (кстати, Коробов для этого же соревнования сделал библиотеку для приведения разных тагсетов к одному): синтаксис rnnmorph очень похож на pymorphy2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22a985d7-1880-457f-ae06-95a7099c8232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 12:00:12.010260: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-06 12:00:12.010289: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-06 12:00:13.910063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-06 12:00:13.910109: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-06 12:00:13.910134: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (aslin): /proc/driver/nvidia/version does not exist\n",
      "2022-04-06 12:00:13.910346: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<normal_form=пердикк; word=Пердикка; pos=NOUN; tag=Case=Acc|Gender=Masc|Number=Sing; score=0.7922>,\n",
       " <normal_form=не; word=не; pos=PART; tag=_; score=1.0000>,\n",
       " <normal_form=менее; word=менее; pos=ADV; tag=Degree=Cmp; score=1.0000>,\n",
       " <normal_form=десять; word=десяти; pos=NUM; tag=Case=Gen; score=1.0000>,\n",
       " <normal_form=раз; word=раз; pos=NOUN; tag=Case=Gen|Gender=Masc|Number=Plur; score=0.9998>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "\n",
    "predictor = RNNMorphPredictor(language='ru')\n",
    "parse = predictor.predict(['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.'])\n",
    "parse[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987dcc7-03fb-4122-8a19-62daabc98f2c",
   "metadata": {},
   "source": [
    "rnnmorph принимает список строк (токенов) и возвращает список объектов специального класса, у которого есть атрибуты normal_form (лемма), word (исходная форма), pos (часть речи), tag (грам. информация) и score (уверенность нейронной модели в правильности своего ответа). Он умеет снимать омонимию (лучше, чем майстем, но хуже, чем интегральный морфопарсер). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "805d158d-f055-413e-958d-e671add5ab92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[-2].pos  # например, можно узнать часть речи для предпоследнего слова в списке"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d73d1-3732-4871-87c1-8e72e27d08e6",
   "metadata": {},
   "source": [
    "*Примечание*\n",
    "\n",
    "RNNMorph, как и любая нейронная модель, умеет работать на GPU (видеокарте). \n",
    "\n",
    "Для любопытных: нейронные сети &ndash; это, по сути, бесконечное умножение гигантских матриц друг на друга. Нейронная сеть состоит из кучи нейронов-функций, и когда она должна выдать ответ, она получает входные данные в виде таких же матриц на первый слой, который состоит из миллиона нейронов, этот миллион нейрончиков кидается высчитывать результат зашитой внутри них функции (что-то похожее на ax + b), а их ответы получает второй слой таких же нейрончиков, и так пока не получится финальный ответ. То есть, мы производим миллиарды однотипных вычислений. Процессор видеокарты устроен как раз таким образом, что он супер-быстро умеет считать однотипные вещи (он считает их батчами &ndash; сразу пачками), то есть, он работает гораздо быстрее, чем CPU, но обучен именно однотипно считать. Поэтому нейронки обычно и работают на GPU, они просто созданы друг для друга!\n",
    "\n",
    "Когда нейронка (и RNNMorph тоже) работает на видеокарте, она делает это обычно быстрее, чем на CPU. Но чтобы заставить RNNMorph перейти на видеокарту, нужно сложно настраивать ее (и иметь совместимую видеокарту до кучи); поэтому RNNMorph умеет, конечно, работать и на обычном процессоре, но при этом вываливает предупреждения про то, что настройки видеокарты он не нашел (они выше выделены красным). Их можно смело игнорировать! Это предупреждения, а не ошибки. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55843f5b-f468-4005-938f-dc6994724228",
   "metadata": {},
   "source": [
    "Последнее, о чем мы поговорили - это формат разметки Universal Dependencies и UDPipe. Из питона запускать UDPipe поучимся в следующий раз, но у него есть версия онлайн на [сайте](https://lindat.mff.cuni.cz/services/udpipe/). \n",
    "\n",
    "[Universal Dependencies](https://universaldependencies.org/) - это грандиозный существующий с 2006 года проект (сперва чешских, а потом и самых разных лингвистов, у нас в России им активно занимается О. Ляшевская), который ставит целью разработать такой формат морфосинтаксической разметки, который был бы одинаково применим к самым разным языкам. То есть, основная его фича - это *единообразие*, из-за чего, к примеру, принято решение в русском языке частицу \"не\" считать advmod (так она себя ведет в германских языках...), а копулу не считать вершиной (потому что копула в агглютинативных языках обычно отсутствует, ср. русское \"петя был учителем\" vs турецкое \"Petya öğretmendi\", где -di - показатель прошедшего времени, присоединяющийся к *существительному*). \n",
    "\n",
    "UD для разметки использует формат файлов .conllu, которые представляют собой таблички (мы с таким на прошлых семинарах имели дело уже). В этом формате существует 10 колонок, каждая ячейка в строке отделяется знаком табуляции; предложения разделяются пустой строкой. На самом сайте UD очень много полезной информации, в том числе описание этого формата и сборник ссылок на приложения, которыми его удобно читать!\n",
    "\n",
    "В питоне мы пытались написать некую читалку формата сами, но на самом деле, конечно, уже существуют готовые библиотеки для этого (и не одна, но мы посмотрим одну). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874ba04-73ca-44b6-b393-c3a62c89b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll\n",
    "\n",
    "text = pyconll.load_from_file('myfile.conllu')\n",
    "\n",
    "for sentence in text:\n",
    "    for token in sentence:\n",
    "        print(token.id, token.form, token.lemma, token.upos, token.feats, token.head, token.deprel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62478cf-e10c-4f9b-980d-f349a6642230",
   "metadata": {},
   "source": [
    "Естественно, можно не только печатать информацию, но и добавлять в список и вообще делать все, что угодно. Что это за атрибуты у токенов?\n",
    "\n",
    "- id - порядковый номер токена в предложении\n",
    "- form - исходная форма\n",
    "- lemma - лемма\n",
    "- upos - часть речи в UD\n",
    "- xpos - часть речи в неуниверсальном формате (обычно встречается, если датасет конвертированный)\n",
    "- feats - грам. характеристики\n",
    "- head - расстояние от синтаксической вершины\n",
    "- deprel - тип синтаксической связи\n",
    "- две зарезервированные ячейки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e768ecb-a67a-4b1b-8915-4a8645fa5832",
   "metadata": {},
   "source": [
    "Где можно красивенько отрисовать .conllu файлы в виде деревьев зависимости:\n",
    "\n",
    "[Арборатор](https://arborator.ilpga.fr/q.cgi): достаточно вставить текст в формате .conllu\n",
    "\n",
    "[Conllu-Viewer на сайте UD](https://universaldependencies.org/conllu_viewer.html): умеет читать файлы и рисовать последовательно все предложения\n",
    "\n",
    "Для затравки вот картинка с арборатора:\n",
    "\n",
    "<img src='arbo.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
