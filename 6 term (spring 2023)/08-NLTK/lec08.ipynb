{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d885b9d8-7952-46d7-b2ea-50320b9cad0f",
   "metadata": {},
   "source": [
    "#### NLTK\n",
    "\n",
    "NLTK - одна из самых первых библиотек такого рода; она огромная и содержит очень много разных инструментов, некоторые из них никак не связаны между собой (в отличие от spacy). NLTK - больше исследовательская библиотека, конструктор своего рода. Для NLTK есть учебник, написанный авторами: [NLTK book](https://www.nltk.org/book/). Для этого учебника специально существует подмодуль book, который обычно импортируется целиком:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5ec6b6-c386-4528-9e01-1fe878512225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd5192a-4ac0-4042-a6c7-e36181e488c6",
   "metadata": {},
   "source": [
    "В этом модуле есть некий набор текстов и набор предложений, с которыми можно поиграться. \n",
    "\n",
    "Центральный объект для NLTK (по крайней мере, при работе с корпусами) - это Text (nltk.text.Text). По сути, в этом объекте содержится сам текст в виде списка токенов, но у него есть дополнительные методы. Что можно делать с объектом класса Text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09bc1861-3616-4962-8149-16e1266fb25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 84 matches:\n",
      "[ Moby Dick by Herman Melville 1851 ] ETYMOLOGY . ( Su\n",
      "hat white whale must be the same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab . \" Do ye k\n",
      " must be the same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab . \" Do ye know the white w\n",
      "ib in a squall . Death and devils ! men , it is Moby Dick ye have seen -- Moby Dick -- Moby Dick !\" \n",
      " devils ! men , it is Moby Dick ye have seen -- Moby Dick -- Moby Dick !\" \" Captain Ahab ,\" said Sta\n"
     ]
    }
   ],
   "source": [
    "text1.concordance('Moby', width=100, lines=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59209f-e284-48dc-9ae7-54ccbffb4a9a",
   "metadata": {},
   "source": [
    "Конкорданс ищет первые n вхождений заданного слова с шириной контекста width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1c743c-eb87-42f1-9d7c-cf5eeaebdb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship boat sea time captain world man deck pequod other whales air\n",
      "water head crew line thing side way body\n"
     ]
    }
   ],
   "source": [
    "text1.similar('whale', num=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea4c47-d413-40f7-ab5b-982c7dc5b0b9",
   "metadata": {},
   "source": [
    "similar возвращает num слов, которые встречаются в похожих контекстах (дистрибутивная похожесть). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7df4b93-207b-4c2c-bc8d-a34d219d4dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_s the_and the_is the_in the_the the_as the_was the_which the_i\n",
      "a_in the_has the_when the_had the_with the_to the_by the_so the_that\n",
      "the_would the_a\n"
     ]
    }
   ],
   "source": [
    "text1.common_contexts(['whale', 'ship'])  # тоже можно задать параметр num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93fa1f-eadf-4fd0-b6b1-3b92ea5605b9",
   "metadata": {},
   "source": [
    "common_contexts ищет те самые совпадающие контексты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fb6f58-a776-4ef3-94ba-df8cec1d2eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text1.dispersion_plot(['Ahab', 'Ishmael'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4f756-62c3-44fd-bba5-44ddd63444dd",
   "metadata": {},
   "source": [
    "Можно построить график распределения слов по тексту (без NumPy и matplotlib график не работает, поэтому установите эти две библиотеки, если еще не). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f11b027-62ad-4cae-985d-21c7a5527856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.count('Moby')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03863563-5aa5-42b4-8555-06fb423e0727",
   "metadata": {},
   "source": [
    "Можно посчитать количество вхождений какого-то слова. Кстати, к текстам можно применять обычные функции len(), set() и подобные. И срезы с индексами работают!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eedb9a4f-8747-4d59-bc77-82250d0e25a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building ngram index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long , from one to the top - mast , and no coffin and went out a sea\n",
      "captain -- this peaking of the whales . , so as to preserve all his\n",
      "might had in former years abounding with them , they toil with their\n",
      "lances , strange tales of Southern whaling . at once the bravest\n",
      "Indians he was , after in vain strove to pierce the profundity . ?\n",
      "then ?\" a levelled flame of pale , And give no chance , watch him ;\n",
      "though the line , it is to be gainsaid . have been\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'long , from one to the top - mast , and no coffin and went out a sea\\ncaptain -- this peaking of the whales . , so as to preserve all his\\nmight had in former years abounding with them , they toil with their\\nlances , strange tales of Southern whaling . at once the bravest\\nIndians he was , after in vain strove to pierce the profundity . ?\\nthen ?\" a levelled flame of pale , And give no chance , watch him ;\\nthough the line , it is to be gainsaid . have been'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.generate(length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac2c4e-299e-414d-8038-2c197b968386",
   "metadata": {},
   "source": [
    "Можно сгенерировать текст, \"похожий\" на оригинальный. Это делается с помощью n-грамов (или n-грамм, я видела разные варианты по-русски...): nltk просто в случайном порядке совмещает эти n-грамы. Как можно видеть, не слишком полезный метод, однако можно побаловаться. \n",
    "\n",
    "Важнее то, что в nltk есть утилиты для работы с n-грамами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "160f8dbc-9fe3-4ebe-8477-dbdb08c66729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('THE', 'suburb', 'of'),\n",
       " ('suburb', 'of', 'Saffron'),\n",
       " ('of', 'Saffron', 'Park'),\n",
       " ('Saffron', 'Park', 'lay'),\n",
       " ('Park', 'lay', 'on'),\n",
       " ('lay', 'on', 'the'),\n",
       " ('on', 'the', 'sunset'),\n",
       " ('the', 'sunset', 'side'),\n",
       " ('sunset', 'side', 'of'),\n",
       " ('side', 'of', 'London'),\n",
       " ('of', 'London', ','),\n",
       " ('London', ',', 'as'),\n",
       " (',', 'as', 'red'),\n",
       " ('as', 'red', 'and'),\n",
       " ('red', 'and', 'ragged'),\n",
       " ('and', 'ragged', 'as'),\n",
       " ('ragged', 'as', 'a'),\n",
       " ('as', 'a', 'cloud'),\n",
       " ('a', 'cloud', 'of'),\n",
       " ('cloud', 'of', 'sunset'),\n",
       " ('of', 'sunset', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams # bigrams\n",
    "\n",
    "list(ngrams(sent9, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82b919-12ec-4e20-9edb-0da6d15cfeb1",
   "metadata": {},
   "source": [
    "Функция ngrams (или bigrams) возвращает список всех н-грамов списка токенов, который ей дать. N-грамы еще принимают число n. \n",
    "\n",
    "У класса Text есть метод, который возвращает коллокации (частотные н-грамы):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b105a78-b99b-417c-aa6b-e85962657a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
      "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
      "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
      "mate; white whale; ivory leg; one hand\n"
     ]
    }
   ],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f51b2b0-7db7-46a1-85ae-83967e0d7ef4",
   "metadata": {},
   "source": [
    "Как создать собственный объект класса Text? Достаточно токенизировать свой текст (любым токенизатором) и передать его в класс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2114b22c-a2c8-4d42-8990-89448287dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc2974-f898-4759-9fad-30de73bede94",
   "metadata": {},
   "source": [
    "Гораздо чаще на практике, однако, используются какие-то отдельные инструменты NLTK, предназначенные для обработки текста. Часть мы уже знаем: это токенизаторы и сентенайзер. \n",
    "\n",
    "Следующий шаг при обработке текстов - это обычно приведение слов к словарной форме. Это уже не такая простая задача, с помощью регулярок ее не решить, приходится использовать либо словари, либо более сложные методы (нейронные сети...)\n",
    "\n",
    "Еще в прошлом веке, правда, придумали (конечно же, для английского языка) более простой способ хоть как-то унифицировать разные словоформы одного слова: стемминг. \n",
    "\n",
    "Стемминг – это уже чисто историческое, можно сказать, явление: в 1980-х, когда еще не было даже графического интерфейса у компьютеров и тем более средств автоматического морфоразбора, Мартин Портер разработал свой алгоритм стемминга: усечение окончания от псевдоосновы. Этот алгоритм так и называется \"стеммер Портера\" и доступен в версиях для нескольких европейских языков, в т.ч. для русского (Snowball – чуть более новая версия). Алгоритм с помощью правил отсекает окончания и суффиксы, основываясь на особенностях языка. Как все правиловое, работает не без ошибок.\n",
    "\n",
    "В NLTK есть несколько стеммеров, а именно:\n",
    "\n",
    "1. PorterStemmer\n",
    "2. SnowballStemmer\n",
    "3. LancasterStemmer\n",
    "4. RegexpStemmer\n",
    "5. RSLPStemmer\n",
    "\n",
    "Все смотреть не будем, можете сами поискать, если интересно, нам хватит двух. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff77802-307b-40b4-be5e-38e9fc347c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "for w in example_words:\n",
    " print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19e91bf-be7a-4376-bc4a-a4a0b607dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пердикк\n",
      "не\n",
      "мен\n",
      "десят\n",
      "раз\n",
      "заключа\n",
      "и\n",
      "расторга\n",
      "союз\n",
      "с\n",
      "основн\n",
      "участник\n",
      "войн\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')  # экземпляр класса \n",
    "example = ['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.']\n",
    "for token in example:\n",
    "    print(stemmer.stem(token))  # stem() - метод класса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a9fb23-a579-4cb3-a3d2-4b3541cda5fa",
   "metadata": {},
   "source": [
    "Другой, более крутой способ унифицировать словоформы - все-таки приводить их к словарной форме. В NLTK есть WordNetLemmatizer, который ищет нужные леммы в словаре. Работает только для английского. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef6d0ba-9083-4a5b-81a4-3b2a89e6abec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n",
      "play\n",
      "play\n",
      "playing\n",
      "playing\n",
      "playing\n",
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('increases'))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7b640-55d2-41a6-b966-f6236cdc154c",
   "metadata": {},
   "source": [
    "При обработке текстов для решения задач NLP, особенно если используются классические алгоритмы машинного обучения (современные нейронки извлекают признаки из всего, им часто лучше, чтобы текст сохранялся в исходном виде), бывает нужно выкинуть слишком распространенные и малозначимые слова: союзы, предлоги и т.п. В NLTK есть списки таких слов для разных языков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e778a4c5-b6c4-4103-ad5c-5a5751676a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стоп-слова: {'зачем', 'бы', 'сам', 'другой', 'есть', 'была', 'много', 'под', 'чуть', 'да', 'при', 'между', 'всегда', 'же', 'сейчас', 'ним', 'тем', 'а', 'по', 'ну', 'эти', 'с', 'было', 'от', 'ей', 'будто', 'один', 'тебя', 'этом', 'какая', 'и', 'через', 'чтоб', 'три', 'вдруг', 'к', 'тот', 'совсем', 'был', 'разве', 'него', 'для', 'хорошо', 'они', 'вас', 'том', 'потому', 'над', 'нельзя', 'быть', 'этот', 'со', 'себе', 'опять', 'всего', 'мне', 'какой', 'ее', 'ничего', 'перед', 'где', 'на', 'когда', 'более', 'меня', 'без', 'о', 'кто', 'мы', 'нас', 'может', 'куда', 'ему', 'эту', 'моя', 'что', 'за', 'лучше', 'чтобы', 'можно', 'им', 'ж', 'во', 'его', 'того', 'этой', 'конечно', 'такой', 'чего', 'них', 'наконец', 'в', 'уж', 'здесь', 'иногда', 'ней', 'вы', 'два', 'еще', 'там', 'после', 'нибудь', 'не', 'из', 'нет', 'ли', 'если', 'никогда', 'этого', 'больше', 'были', 'теперь', 'про', 'до', 'почти', 'раз', 'я', 'так', 'но', 'ни', 'тогда', 'или', 'себя', 'все', 'мой', 'она', 'ведь', 'вот', 'только', 'хоть', 'чем', 'ты', 'об', 'нее', 'свою', 'их', 'он', 'вам', 'тут', 'надо', 'потом', 'у', 'как', 'будет', 'то', 'даже', 'всех', 'всю', 'уже', 'впрочем', 'тоже'}\n",
      "['обработке', 'текстов', 'решения', 'задач', 'NLP', ',', 'особенно', 'используются', 'классические', 'алгоритмы', 'машинного', 'обучения', '(', 'современные', 'нейронки', 'извлекают', 'признаки', ',', 'часто', ',', 'текст', 'сохранялся', 'исходном', 'виде', ')', ',', 'бывает', 'нужно', 'выкинуть', 'слишком', 'распространенные', 'малозначимые', 'слова', ':', 'союзы', ',', 'предлоги', 'т.п', '.', 'NLTK', 'списки', 'таких', 'слов', 'разных', 'языков']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = 'При обработке текстов для решения задач NLP, особенно если используются классические алгоритмы машинного обучения (современные нейронки извлекают признаки из всего, им часто лучше, чтобы текст сохранялся в исходном виде), бывает нужно выкинуть слишком распространенные и малозначимые слова: союзы, предлоги и т.п. В NLTK есть списки таких слов для разных языков'\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "print('Стоп-слова:', stop_words)\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6e60b-d487-4d8b-9508-3e738f47a4da",
   "metadata": {},
   "source": [
    "Наконец к морфологии. Подробнее о ней мы поговорим в следующие разы, потому что морфологический разбор (с частями речи и еще и грамматическими характеристиками) - еще более сложная задача. В NLTK реализованы простейшие статистические парсеры, которые можно самостоятельно обучать на данных размеченных корпусов NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7860d712-f7fa-4447-8dea-fb925ee4b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = word_tokenize(\"And now for something completely different\")\n",
    "print(nltk.pos_tag(text))\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "623758d8-01e1-4408-a7b4-26428b769408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PPSS'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('analyze', None),\n",
       " ('sentences', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('NLTK', None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown # Брауновский корпус\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news') # возьмем все размеченные предложения из новостей\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents) # натренируем таггер на размеченных предложениях (он просто посчитает статистику)\n",
    "unigram_tagger.tag(word_tokenize('I want to analyze sentences with NLTK')) # попробуем на предложении, которого он не видел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e17494a3-86d8-4172-af7d-7fe4fb77df51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PPSS'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('analyze', None),\n",
       " ('sentences', None),\n",
       " ('with', None),\n",
       " ('NLTK', None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(brown_tagged_sents)\n",
    "bigram_tagger.tag(word_tokenize('I want to analyze sentences with NLTK'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e62fcc9-2617-44df-96ec-fafcf57bdfc1",
   "metadata": {},
   "source": [
    "Bigram tagger работает как будто бы хуже, чем unigram, но в теории он может лучше справляться с омонимией (потому что учитывает рядом стоящее слово, и тогда beautiful book vs book something не разберет одинаково). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36de76-28be-444a-b7cf-5ccc09d493a5",
   "metadata": {},
   "source": [
    "Также NLTK умеет работать с данными базы [WordNet](https://wordnet.princeton.edu/). Оттуда он автоматически может извлекать сведения о семантических отношениях между словами, а также на данных WordNet у него реализован алгоритм Леска для решения задачи Word Sense Disambiguation. \n",
    "\n",
    "Это очень известная задача в NLP-мире, можно про нее подробнее посмотреть на [nlpprogress](http://nlpprogress.com/english/word_sense_disambiguation.html). Для ее решения мы должны неоднозначным словам в контекстах сопоставить дефиниции из словаря (в роли какового для английского языка успешно выступает WordNet). Алгоритм Леска был придуман в 1986 году и считается классическим подходом (бейзлайн, ага) для решения этой задачи. Мы предполагаем, что слова в заданном окне контекста (среди окружающих их слов) будут иметь похожую тематику. Это еще называется **дистрибутивная гипотеза** (\"Лингвистические единицы, встречающиеся в схожих контекстах, имеют близкие значения.\", придумали это лингвисты уже много лет назад). По алгоритму Леска, определение в словаре для целевого слова сравнивается со словами, которые стоят вокруг него в контексте. \n",
    "\n",
    "В базовой имплементации алгоритм Леска делает следующее:\n",
    "\n",
    "- считает количество слов, стоящих рядом с искомым словом и оказавшихся в словарном определении слова (для каждого варианта определения)\n",
    "- Каких слов больше всего оказалось, то и значение. \n",
    "\n",
    "Очень просто!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b2a8817-9428-4295-a49a-7a326c4da0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "def get_semantic(seq, key_word):\n",
    "    temp = word_tokenize(seq)\n",
    "    temp = lesk(temp, key_word)\n",
    "    return temp.definition() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93507a61-9a02-4a73-a4a1-da8b03794de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrange for and reserve (something for someone else) in advance\n",
      "a number of sheets (ticket or stamps etc.) bound together on one edge\n"
     ]
    }
   ],
   "source": [
    "print(get_semantic('The table was already booked by someone else', 'book'))\n",
    "print(get_semantic('I love reading books on programming', 'book'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb628a77-dd06-4307-aac5-5ec2909e90bd",
   "metadata": {},
   "source": [
    "Ну и синонимы с антонимами просто собираются напрямую из WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "250651dd-432e-455d-b890-269d6795ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms ['dog.n.01', 'frump.n.01', 'dog.n.03', 'cad.n.01', 'frank.n.02', 'pawl.n.01', 'andiron.n.01', 'chase.v.01']\n",
      "['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "for syns in wordnet.synsets('dog'):\n",
    " synonyms.append(syns.name())\n",
    "print (\"synonyms\", synonyms)\n",
    "\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    " for l in syn.lemmas():\n",
    "  if l.antonyms():\n",
    "   antonyms.append(l.antonyms()[0].name())\n",
    "print(antonyms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
